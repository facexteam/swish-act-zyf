{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y = mx.nd.array([1.0])\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwishAct(mx.operator.CustomOp):\n",
    "    def __init__(self, beta=1.0):\n",
    "        self._beta = beta\n",
    "        self._x_sig = None\n",
    "        \n",
    "    def forward(self, is_train, req, in_data, out_data, aux):\n",
    "        \"\"\"Implements forward computation.\n",
    "\n",
    "        is_train : bool, whether forwarding for training or testing.\n",
    "        req : list of {'null', 'write', 'inplace', 'add'}, how to assign to out_data. 'null' means skip assignment, etc.\n",
    "        in_data : list of NDArray, input data.\n",
    "        out_data : list of NDArray, pre-allocated output buffers.\n",
    "        aux : list of NDArray, mutable auxiliary states. Usually not used.\n",
    "        \"\"\"\n",
    "        print '===> In forward(): '\n",
    "        \n",
    "        print '---> before calc: '\n",
    "        print 'is_train: ', is_train\n",
    "        print 'req: ', req\n",
    "        print 'in_data: ', in_data\n",
    "        print 'out_data: ', out_data\n",
    "        print 'aux: ', aux\n",
    "        \n",
    "        x = in_data[0]\n",
    "        self._x_sig = mx.nd.sigmoid(x * self._beta)\n",
    "        y = x * self._x_sig\n",
    "        self.assign(out_data[0], req[0], y)\n",
    "        \n",
    "        print '---> after calc: '\n",
    "#         print 'is_train: ', is_train\n",
    "#         print 'req: ', req\n",
    "#         print 'in_data: ', in_data\n",
    "        print 'out_data: ', out_data\n",
    "#         print 'aux: ', aux    \n",
    "        \n",
    "\n",
    "    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n",
    "        \"\"\"Implements backward computation\n",
    "\n",
    "        req : list of {'null', 'write', 'inplace', 'add'}, how to assign to in_grad\n",
    "        out_grad : list of NDArray, gradient w.r.t. output data.\n",
    "        in_grad : list of NDArray, gradient w.r.t. input data. This is the output buffer.\n",
    "        \"\"\"\n",
    "        print '===> In backward(): '\n",
    "        \n",
    "        print '---> before calc: '\n",
    "        print 'req: ', req\n",
    "        print 'out_grad: ', out_grad\n",
    "        print 'in_data: ', in_data\n",
    "        print 'out_data: ', out_data\n",
    "        print 'in_grad: ', in_grad\n",
    "        print 'aux: ', aux\n",
    "        \n",
    "        x = in_data[0]\n",
    "        y = out_data[0]\n",
    "        dy = out_grad[0]\n",
    "        print 'dy in backward(): ', dy\n",
    "        dx = dy * (y * self._beta + self._x_sig*(1.0 - y * self._beta))\n",
    "        self.assign(in_grad[0], req[0], dx)\n",
    "        \n",
    "        print '---> after calc: '\n",
    "#         print 'req: ', req\n",
    "#         print 'out_grad: ', out_grad\n",
    "#         print 'in_data: ', in_data\n",
    "#         print 'out_data: ', out_data\n",
    "#         print 'in_grad: ', in_grad\n",
    "        print 'aux: ', aux        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mx.operator.register(\"SwishAct\")  # register with name \"sigmoid\"\n",
    "class SwishActProp(mx.operator.CustomOpProp):\n",
    "    def __init__(self, beta=1.0):\n",
    "        super(SwishActProp, self).__init__(True)\n",
    "        # we use constant bias here to illustrate how to pass arguments\n",
    "        # to operators. All arguments are in string format so you need\n",
    "        # to convert them back to the type you want.\n",
    "        self._beta = float(beta)\n",
    "\n",
    "    def list_arguments(self):\n",
    "        return ['data']\n",
    "\n",
    "    def list_outputs(self):\n",
    "        #  this can be omitted if you only have 1 output.\n",
    "        return ['output']\n",
    "\n",
    "    def infer_shape(self, in_shapes):\n",
    "        \"\"\"Calculate output shapes from input shapes. This can be\n",
    "        omited if all your inputs and outputs have the same shape.\n",
    "\n",
    "        in_shapes : list of shape. Shape is described by a tuple of int.\n",
    "        \"\"\"\n",
    "        data_shape = in_shapes[0]\n",
    "        output_shape = data_shape\n",
    "        # return 3 lists representing inputs shapes, outputs shapes, and aux data shapes.\n",
    "        return (data_shape,), (output_shape,), ()\n",
    "\n",
    "    def create_operator(self, ctx, in_shapes, in_dtypes):\n",
    "        #  create and return the CustomOp class.\n",
    "        return SwishAct(self._beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--->input\n",
      "('beta=', 2.0)\n",
      "('x=', \n",
      "[[-5. -4. -3. -2.]\n",
      " [-1.  0.  1.  2.]\n",
      " [ 3.  4.  5.  6.]]\n",
      "<NDArray 3x4 @cpu(0)>)\n",
      "('x.grad=', \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "<NDArray 3x4 @cpu(0)>)\n",
      "--->forwarding\n",
      "===> In forward(): \n",
      "---> before calc: \n",
      "is_train:  1\n",
      " --->after forwardingreq:  ['write']\n",
      "in_data:  \n",
      "[\n",
      "[[-5. -4. -3. -2.]\n",
      " [-1.  0.  1.  2.]\n",
      " [ 3.  4.  5.  6.]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "out_data:  [\n",
      "[[ 0.0000000e+00  0.0000000e+00  2.5223372e-44  0.0000000e+00]\n",
      " [           nan  0.0000000e+00  2.8168788e+20  2.9206201e+32]\n",
      " [ 4.6280266e+27  7.2151485e+22 -5.6501617e+14  4.5657107e-41]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "aux:  []\n",
      "---> after calc: \n",
      "out_data:  [\n",
      "[[-2.2698936e-04 -1.3414006e-03 -7.4178688e-03 -3.5972420e-02]\n",
      " [-1.1920292e-01  0.0000000e+00  8.8079703e-01  1.9640275e+00]\n",
      " [ 2.9925823e+00  3.9986587e+00  4.9997730e+00  5.9999628e+00]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "('y=', \n",
      "[[-2.2698936e-04 -1.3414006e-03 -7.4178688e-03 -3.5972420e-02]\n",
      " [-1.1920292e-01  0.0000000e+00  8.8079703e-01  1.9640275e+00]\n",
      " [ 2.9925823e+00  3.9986587e+00  4.9997730e+00  5.9999628e+00]]\n",
      "<NDArray 3x4 @cpu(0)>)\n",
      "('y.grad=', None)\n",
      "--->backwarding\n",
      "===> In backward(): --->after backwarding\n",
      "\n",
      "---> before calc: \n",
      "req:  ['write']\n",
      "out_grad:  [\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "in_data:  [\n",
      "[[-5. -4. -3. -2.]\n",
      " [-1.  0.  1.  2.]\n",
      " [ 3.  4.  5.  6.]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "out_data:  [\n",
      "[[-2.2698936e-04 -1.3414006e-03 -7.4178688e-03 -3.5972420e-02]\n",
      " [-1.1920292e-01  0.0000000e+00  8.8079703e-01  1.9640275e+00]\n",
      " [ 2.9925823e+00  3.9986587e+00  4.9997730e+00  5.9999628e+00]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "in_grad:  [\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "aux:  []\n",
      "dy in backward():  \n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "<NDArray 3x4 @cpu(0)>\n",
      "---> after calc: \n",
      "aux:  []\n",
      "('x.grad=', \n",
      "[[-4.0856024e-04 -2.3465513e-03 -1.2326431e-02 -5.2664615e-02]\n",
      " [-9.0784252e-02  5.0000000e-01  1.0907843e+00  1.0526648e+00]\n",
      " [ 1.0123262e+00  1.0023465e+00  1.0004091e+00  1.0000677e+00]]\n",
      "<NDArray 3x4 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "x = mx.nd.array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6])\n",
    "x = x.reshape((3,4))\n",
    "\n",
    "beta = 2.0 \n",
    "# attach gradient buffer to x for autograd\n",
    "x.attach_grad()\n",
    "print('--->input') \n",
    "print('beta=', beta)\n",
    "print('x=', x)\n",
    "print('x.grad=', x.grad)\n",
    "\n",
    "print('--->forwarding') \n",
    "# forward in a record() section to save computation graph for backward\n",
    "# see autograd tutorial to learn more.\n",
    "with autograd.record():\n",
    "    y = mx.nd.Custom(x, beta=beta, op_type='SwishAct')\n",
    "\n",
    "print('--->after forwarding') \n",
    "print('y=', y)\n",
    "print('y.grad=', y.grad)\n",
    "\n",
    "print('--->backwarding') \n",
    "y.backward()\n",
    "print('--->after backwarding') \n",
    "print('x.grad=', x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> In forward(): \n",
      "---> before calc: \n",
      " is_train:  0\n",
      "req:  ['write']\n",
      "in_data:  [\n",
      "[[-5. -4. -3. -2.]\n",
      " [-1.  0.  1.  2.]\n",
      " [ 3.  4.  5.  6.]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "out_data:  [\n",
      "[[0.0000000e+00 0.0000000e+00 2.5223372e-44 0.0000000e+00]\n",
      " [          nan 4.5655705e-41 2.8168788e+20 2.9206201e+32]\n",
      " [4.6280266e+27 7.2151485e+22 2.0073407e+00 6.0000000e+00]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "aux:  []\n",
      "---> after calc: \n",
      "out_data:  [\n",
      "[[-0.03346425 -0.07194484 -0.14227763 -0.23840584]\n",
      " [-0.26894143  0.          0.7310586   1.761594  ]\n",
      " [ 2.8577223   3.928055    4.966536    5.9851646 ]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "===> In forward(): \n",
      "---> before calc: \n",
      "is_train:  0\n",
      "req:  \n",
      "[[-0.03346425 -0.07194484 -0.14227763 -0.23840584]\n",
      " [-0.26894143  0.          0.7310586   1.761594  ]\n",
      " [ 2.8577223   3.928055    4.966536    5.9851646 ]]\n",
      "<NDArray 3x4 @cpu(0)>['write']\n",
      "in_data:  \n",
      "[\n",
      "[[-5. -4. -3. -2.]\n",
      " [-1.  0.  1.  2.]\n",
      " [ 3.  4.  5.  6.]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "out_data:  [\n",
      "[[0.0000000e+00 0.0000000e+00 2.5223372e-44 0.0000000e+00]\n",
      " [          nan 0.0000000e+00 2.8168788e+20 2.9206201e+32]\n",
      " [4.6280266e+27 7.2151485e+22 4.3144578e-41 0.0000000e+00]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "aux:  []\n",
      "---> after calc: \n",
      "out_data:  [\n",
      "[[-2.2698936e-04 -1.3414006e-03 -7.4178688e-03 -3.5972420e-02]\n",
      " [-1.1920292e-01  0.0000000e+00  8.8079703e-01  1.9640275e+00]\n",
      " [ 2.9925823e+00  3.9986587e+00  4.9997730e+00  5.9999628e+00]]\n",
      "<NDArray 3x4 @cpu(0)>]\n",
      "\n",
      "[[-2.2698936e-04 -1.3414006e-03 -7.4178688e-03 -3.5972420e-02]\n",
      " [-1.1920292e-01  0.0000000e+00  8.8079703e-01  1.9640275e+00]\n",
      " [ 2.9925823e+00  3.9986587e+00  4.9997730e+00  5.9999628e+00]]\n",
      "<NDArray 3x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# x = mx.nd.array([0, 1, 2, 3])\n",
    "x = mx.nd.array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6])\n",
    "x = x.reshape((3,4))\n",
    "\n",
    "# attach gradient buffer to x for autograd\n",
    "# x.attach_grad()\n",
    "# forward in a record() section to save computation graph for backward\n",
    "# see autograd tutorial to learn more.\n",
    "# with autograd.record():\n",
    "y = mx.nd.Custom(x, beta=1.0, op_type='SwishAct')\n",
    "y2 = mx.nd.Custom(x, beta=2.0, op_type='SwishAct')\n",
    "\n",
    "print(y)\n",
    "print(y2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
